"""
Evaluation Page for Agent RAG Studio
"""
import streamlit as st
import logging
import pandas as pd
from datetime import datetime
import json
import traceback
from typing import Dict, Any, List
import os

# Configure logging
logger = logging.getLogger(__name__)

# Import our modules
try:
    from rag.config import get_config
except ImportError as e:
    st.error(f"ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    st.stop()

def initialize_session_state():
    """Initialize session state for evaluation"""
    if 'eval_initialized' not in st.session_state:
        st.session_state.eval_initialized = True
        st.session_state.eval_datasets = []
        st.session_state.eval_results = []
        st.session_state.eval_history = []


def extract_chat_history_for_evaluation():
    """Extract questions from chat history for evaluation"""
    if not hasattr(st.session_state, 'messages') or not st.session_state.messages:
        return []
    
    evaluation_data = []
    
    # Extract user questions and assistant answers
    for i in range(len(st.session_state.messages) - 1):
        current_msg = st.session_state.messages[i]
        next_msg = st.session_state.messages[i + 1]
        
        # Check if current message is user question and next is assistant answer
        if (current_msg.get('role') == 'user' and 
            next_msg.get('role') == 'assistant'):
            
            question = current_msg.get('content', '').strip()
            answer = next_msg.get('content', '').strip()
            
            if question and answer:
                evaluation_data.append({
                    "question": question,
                    "expected_answer": answer,  # Use actual answer as reference
                    "category": "ãƒãƒ£ãƒƒãƒˆå±¥æ­´",
                    "timestamp": current_msg.get('timestamp', ''),
                    "sources": next_msg.get('sources', []),
                    "metadata": next_msg.get('metadata', {})
                })
    
    return evaluation_data



def create_sample_dataset():
    """Create sample evaluation dataset"""
    sample_questions = [
        {
            "question": "LangChainã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
            "expected_answer": "LangChainã¯å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã‚’æ´»ç”¨ã—ãŸã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºã®ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚",
            "category": "åŸºæœ¬æ¦‚å¿µ"
        },
        {
            "question": "RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆ©ç‚¹ã‚’æ•™ãˆã¦ãã ã•ã„",
            "expected_answer": "RAGã¯æœ€æ–°æƒ…å ±ã¸ã®å¯¾å¿œã€æƒ…å ±ã®æ­£ç¢ºæ€§å‘ä¸Šã€é€æ˜æ€§ã®ç¢ºä¿ãªã©ã®åˆ©ç‚¹ãŒã‚ã‚Šã¾ã™ã€‚",
            "category": "RAG"
        },
        {
            "question": "ãƒ™ã‚¯ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¯ãªãœå¿…è¦ã§ã™ã‹ï¼Ÿ",
            "expected_answer": "ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚’å¯èƒ½ã«ã—ã€é¡ä¼¼ã™ã‚‹æ–‡æ›¸ã‚’åŠ¹ç‡çš„ã«è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã‚‹ãŸã‚ã§ã™ã€‚",
            "category": "æŠ€è¡“"
        },
        {
            "question": "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ã‚³ãƒ„ã¯ï¼Ÿ",
            "expected_answer": "æ˜ç¢ºã§å…·ä½“çš„ãªæŒ‡ç¤ºã€ä¾‹ã®æä¾›ã€æ®µéšçš„ãªèª¬æ˜ãªã©ãŒé‡è¦ã§ã™ã€‚",
            "category": "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"
        },
        {
            "question": "ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ ã®ç‰¹å¾´ã¯ï¼Ÿ",
            "expected_answer": "è‡ªå¾‹çš„ãªæ„æ€æ±ºå®šã€ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã€è¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—ã§ã®å•é¡Œè§£æ±ºãŒå¯èƒ½ã§ã™ã€‚",
            "category": "ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"
        }
    ]
    
    return sample_questions

def run_basic_evaluation(selected_dataset=None):
    """Run basic evaluation without Ragas"""
    if not st.session_state.get('chain'):
        st.error("RAGãƒã‚§ãƒ¼ãƒ³ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return None
    
    # Get test questions based on selected dataset
    if selected_dataset and selected_dataset.startswith("ãƒãƒ£ãƒƒãƒˆå±¥æ­´"):
        chat_history = extract_chat_history_for_evaluation()
        dataset = chat_history
        st.info(f"ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ä½¿ç”¨ ({len(dataset)} è³ªå•)")
    else:
        dataset = create_sample_dataset()
        st.info("ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨")
    
    if not dataset:
        st.warning("è©•ä¾¡ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None
    
    results = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i, item in enumerate(dataset):
        status_text.text(f"è©•ä¾¡ä¸­: {item['question'][:50]}...")
        
        try:
            # Get answer from RAG system
            start_time = datetime.now()
            result = st.session_state.chain.invoke({"question": item["question"]})
            end_time = datetime.now()
            
            response_time = (end_time - start_time).total_seconds()
            
            # Simple evaluation metrics
            answer = result.get("answer", "")
            sources = result.get("sources", [])
            
            # Basic metrics
            has_sources = len(sources) > 0
            answer_length = len(answer)
            source_count = len(sources)
            
            # Simple relevance check (keyword overlap)
            question_words = set(item["question"].lower().split())
            answer_words = set(answer.lower().split())
            keyword_overlap = len(question_words.intersection(answer_words)) / len(question_words) if question_words else 0
            
            # For chat history evaluation, compare with previous answer
            consistency_score = 0.0
            if selected_dataset and selected_dataset.startswith("ãƒãƒ£ãƒƒãƒˆå±¥æ­´"):
                # Calculate consistency with previous answer
                expected_words = set(item["expected_answer"].lower().split())
                consistency_score = len(expected_words.intersection(answer_words)) / len(expected_words) if expected_words else 0
            
            eval_result = {
                "question": item["question"],
                "expected_answer": item["expected_answer"],
                "actual_answer": answer,
                "category": item["category"],
                "response_time": response_time,
                "has_sources": has_sources,
                "source_count": source_count,
                "answer_length": answer_length,
                "keyword_overlap": keyword_overlap,
                "consistency_score": consistency_score,  # New metric for chat history
                "sources": sources,
                "timestamp": datetime.now().isoformat(),
                "evaluation_type": "chat_history" if selected_dataset and selected_dataset.startswith("ãƒãƒ£ãƒƒãƒˆå±¥æ­´") else "standard"
            }
            
            results.append(eval_result)
            
        except Exception as e:
            logger.error(f"Evaluation error for question: {item['question']}: {e}")
            continue
        
        progress_bar.progress((i + 1) / len(dataset))
    
    progress_bar.empty()
    status_text.empty()
    
    return results

def run_ragas_evaluation(selected_dataset=None):
    """Run evaluation using Ragas (if available)"""
    try:
        # Try to import Ragas
        from ragas import evaluate
        from ragas.metrics import (
            faithfulness,
            answer_relevancy,
            context_precision,
            context_recall
        )
        from datasets import Dataset
        
        if not st.session_state.get('chain'):
            st.error("RAGãƒã‚§ãƒ¼ãƒ³ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None
        
        st.info("Ragasè©•ä¾¡ã‚’å®Ÿè¡Œä¸­...")
        
        # Get test questions based on selected dataset
        if selected_dataset and selected_dataset.startswith("ãƒãƒ£ãƒƒãƒˆå±¥æ­´"):
            chat_history = extract_chat_history_for_evaluation()
            dataset = chat_history
            st.info(f"ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ä½¿ç”¨ ({len(dataset)} è³ªå•)")
        else:
            dataset = create_sample_dataset()
            st.info("ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨")
        
        if not dataset:
            st.warning("è©•ä¾¡ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
        
        # Prepare data for Ragas
        questions = []
        answers = []
        contexts = []
        ground_truths = []
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, item in enumerate(dataset):
            status_text.text(f"ãƒ‡ãƒ¼ã‚¿æº–å‚™ä¸­: {item['question'][:50]}...")
            
            try:
                # Get answer and context from RAG system
                result = st.session_state.chain.invoke({"question": item["question"]})
                
                questions.append(item["question"])
                answers.append(result.get("answer", ""))
                
                # Get contexts from sources
                sources = result.get("sources", [])
                context_list = [source.get("snippet", "") for source in sources if source.get("snippet")]
                contexts.append(context_list)
                
                ground_truths.append(item["expected_answer"])
                
            except Exception as e:
                logger.error(f"Error preparing data for question: {item['question']}: {e}")
                continue
            
            progress_bar.progress((i + 1) / len(dataset))
        
        # Create Ragas dataset
        ragas_dataset = Dataset.from_dict({
            "question": questions,
            "answer": answers,
            "contexts": contexts,
            "ground_truth": ground_truths
        })
        
        status_text.text("Ragasè©•ä¾¡ã‚’å®Ÿè¡Œä¸­...")
        
        # Run evaluation
        ragas_result = evaluate(
            ragas_dataset,
            metrics=[
                faithfulness,
                answer_relevancy,
                context_precision,
                context_recall
            ]
        )
        
        progress_bar.empty()
        status_text.empty()
        
        return ragas_result
        
    except ImportError:
        st.warning("RagasãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€åŸºæœ¬è©•ä¾¡ã®ã¿å®Ÿè¡Œã—ã¾ã™")
        return None
    except Exception as e:
        st.error(f"Ragasè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {str(e)}")
        logger.error(f"Ragas evaluation error: {e}")
        return None

def render_evaluation_section():
    """Render evaluation configuration and execution"""
    st.subheader("ğŸ§ª è©•ä¾¡å®Ÿè¡Œ")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### è©•ä¾¡ã‚¿ã‚¤ãƒ—")
        eval_type = st.selectbox(
            "è©•ä¾¡æ–¹æ³•ã‚’é¸æŠ",
            ["åŸºæœ¬è©•ä¾¡", "Ragasè©•ä¾¡", "ä¸¡æ–¹"],
            help="åŸºæœ¬è©•ä¾¡ã¯ç°¡å˜ãªæŒ‡æ¨™ã€Ragasè©•ä¾¡ã¯é«˜åº¦ãªæŒ‡æ¨™ã‚’ä½¿ç”¨"
        )
        
        st.markdown("### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠ")
        chat_history = extract_chat_history_for_evaluation()
        
        dataset_options = ["ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"]
        if chat_history:
            dataset_options.insert(0, f"ãƒãƒ£ãƒƒãƒˆå±¥æ­´ ({len(chat_history)} è³ªå•)")
        
        selected_dataset = st.selectbox(
            "è©•ä¾¡ã«ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            dataset_options,
            help="ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã¾ãŸã¯ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰é¸æŠ"
        )
        
        # Display dataset preview
        if selected_dataset.startswith("ãƒãƒ£ãƒƒãƒˆå±¥æ­´") and chat_history:
            with st.expander("ãƒãƒ£ãƒƒãƒˆå±¥æ­´ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼"):
                preview_df = pd.DataFrame(chat_history[:3])  # Show first 3 items
                st.dataframe(preview_df[['question', 'timestamp']], use_container_width=True)

    
    with col2:
        st.markdown("### è©•ä¾¡è¨­å®š")
        
        include_sources = st.checkbox("ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’å«ã‚ã‚‹", value=True)
        save_results = st.checkbox("çµæœã‚’ä¿å­˜", value=True)
        show_details = st.checkbox("è©³ç´°çµæœã‚’è¡¨ç¤º", value=True)
    
    # Run evaluation button
    if st.button("ğŸš€ è©•ä¾¡ã‚’å®Ÿè¡Œ", type="primary"):
        if not st.session_state.get('index_ready', False):
            st.error("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return
        
        with st.spinner("è©•ä¾¡ã‚’å®Ÿè¡Œä¸­..."):
            results = []
            
            # Run basic evaluation
            if eval_type in ["åŸºæœ¬è©•ä¾¡", "ä¸¡æ–¹"]:
                basic_results = run_basic_evaluation(selected_dataset)
                if basic_results:
                    results.extend(basic_results)
            
            # Run Ragas evaluation
            if eval_type in ["Ragasè©•ä¾¡", "ä¸¡æ–¹"]:
                ragas_results = run_ragas_evaluation(selected_dataset)
                if ragas_results:
                    st.session_state.ragas_results = ragas_results
            
            if results:
                st.session_state.eval_results = results
                st.session_state.eval_history.append({
                    "timestamp": datetime.now().isoformat(),
                    "eval_type": eval_type,
                    "num_questions": len(results),
                    "results": results
                })
                
                if save_results:
                    # Save to file
                    os.makedirs("eval_results", exist_ok=True)
                    filename = f"eval_results/evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                    with open(filename, 'w', encoding='utf-8') as f:
                        json.dump(results, f, ensure_ascii=False, indent=2)
                    st.success(f"çµæœã‚’ {filename} ã«ä¿å­˜ã—ã¾ã—ãŸ")
                
                st.success("âœ… è©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸï¼")

def render_results_section():
    """Render evaluation results"""
    if not st.session_state.eval_results:
        st.info("ã¾ãšè©•ä¾¡ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        return
    
    st.subheader("ğŸ“ˆ è©•ä¾¡çµæœ")
    
    results = st.session_state.eval_results
    
    # Summary metrics
    is_chat_history_eval = any(r.get("evaluation_type") == "chat_history" for r in results)
    
    if is_chat_history_eval:
        col1, col2, col3, col4, col5 = st.columns(5)
    else:
        col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        avg_response_time = sum(r["response_time"] for r in results) / len(results)
        st.metric("å¹³å‡å¿œç­”æ™‚é–“", f"{avg_response_time:.2f}s")
    
    with col2:
        source_coverage = sum(1 for r in results if r["has_sources"]) / len(results)
        st.metric("ã‚½ãƒ¼ã‚¹ä»˜ä¸ç‡", f"{source_coverage:.1%}")
    
    with col3:
        avg_sources = sum(r["source_count"] for r in results) / len(results)
        st.metric("å¹³å‡ã‚½ãƒ¼ã‚¹æ•°", f"{avg_sources:.1f}")
    
    with col4:
        avg_overlap = sum(r["keyword_overlap"] for r in results) / len(results)
        st.metric("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è‡´ç‡", f"{avg_overlap:.1%}")
    
    if is_chat_history_eval:
        with col5:
            avg_consistency = sum(r.get("consistency_score", 0) for r in results) / len(results)
            st.metric("ä¸€è²«æ€§ã‚¹ã‚³ã‚¢", f"{avg_consistency:.1%}")
    
    # Detailed results table
    st.markdown("### è©³ç´°çµæœ")
    
    df_data = []
    for r in results:
        row_data = {
            "è³ªå•": r["question"][:50] + "..." if len(r["question"]) > 50 else r["question"],
            "ã‚«ãƒ†ã‚´ãƒª": r["category"],
            "å¿œç­”æ™‚é–“(s)": f"{r['response_time']:.2f}",
            "ã‚½ãƒ¼ã‚¹æ•°": r["source_count"],
            "å›ç­”é•·": r["answer_length"],
            "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è‡´": f"{r['keyword_overlap']:.1%}",
        }
        
        # Add consistency score for chat history evaluation
        if is_chat_history_eval and "consistency_score" in r:
            row_data["ä¸€è²«æ€§"] = f"{r['consistency_score']:.1%}"
        
        df_data.append(row_data)
    
    df = pd.DataFrame(df_data)
    st.dataframe(df, use_container_width=True)
    
    # Category analysis
    st.markdown("### ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ")
    
    category_stats = {}
    for r in results:
        cat = r["category"]
        if cat not in category_stats:
            category_stats[cat] = {
                "count": 0,
                "avg_time": 0,
                "avg_sources": 0,
                "avg_overlap": 0
            }
        
        stats = category_stats[cat]
        stats["count"] += 1
        stats["avg_time"] += r["response_time"]
        stats["avg_sources"] += r["source_count"]
        stats["avg_overlap"] += r["keyword_overlap"]
    
    # Calculate averages
    for cat, stats in category_stats.items():
        count = stats["count"]
        stats["avg_time"] /= count
        stats["avg_sources"] /= count
        stats["avg_overlap"] /= count
    
    # Display category stats
    for cat, stats in category_stats.items():
        with st.expander(f"ğŸ“Š {cat} ({stats['count']} ä»¶)"):
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("å¹³å‡å¿œç­”æ™‚é–“", f"{stats['avg_time']:.2f}s")
            with col2:
                st.metric("å¹³å‡ã‚½ãƒ¼ã‚¹æ•°", f"{stats['avg_sources']:.1f}")
            with col3:
                st.metric("å¹³å‡ä¸€è‡´ç‡", f"{stats['avg_overlap']:.1%}")

def render_ragas_results():
    """Render Ragas evaluation results"""
    if not st.session_state.get('ragas_results'):
        return
    
    st.subheader("ğŸ¯ Ragasè©•ä¾¡çµæœ")
    
    ragas_results = st.session_state.ragas_results
    
    # Main metrics
    # --- ã“ã“ã‹ã‚‰Ragasã‚¹ã‚³ã‚¢è¡¨ç¤º ---
    faithfulness_score = getattr(ragas_results, "faithfulness", 0)
    relevancy_score = getattr(ragas_results, "answer_relevancy", 0)
    precision_score = getattr(ragas_results, "context_precision", 0)
    recall_score = getattr(ragas_results, "context_recall", 0)
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Faithfulness", f"{faithfulness_score:.3f}")
        st.caption("å›ç­”ã®äº‹å®Ÿæ€§")
    with col2:
        st.metric("Relevancy", f"{relevancy_score:.3f}")
        st.caption("å›ç­”ã®é–¢é€£æ€§")
    with col3:
        st.metric("Precision", f"{precision_score:.3f}")
        st.caption("æ ¹æ‹ ã®ç²¾åº¦")
    with col4:
        st.metric("Recall", f"{recall_score:.3f}")
        st.caption("æ ¹æ‹ ã®ç¶²ç¾…æ€§")
    # --- ã“ã“ã¾ã§Ragasã‚¹ã‚³ã‚¢è¡¨ç¤º ---
    
    # Ragas recommendations
    st.markdown("### ğŸ“‹ æ”¹å–„ææ¡ˆ")
    
    if faithfulness_score < 0.7:
        st.warning("**Faithfulness** ãŒä½ã‚ã§ã™ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§äº‹å®Ÿã®ã¿ã«åŸºã¥ãå›ç­”ã‚’å¼·èª¿ã—ã¦ãã ã•ã„ã€‚")
    
    if relevancy_score < 0.7:
        st.warning("**Answer Relevancy** ãŒä½ã‚ã§ã™ã€‚è³ªå•ã«ã‚ˆã‚Šç›´æ¥çš„ã«å›ç­”ã™ã‚‹ã‚ˆã†ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
    
    if precision_score < 0.7:
        st.warning("**Context Precision** ãŒä½ã‚ã§ã™ã€‚ã‚ˆã‚Šé–¢é€£æ€§ã®é«˜ã„æ–‡æ›¸ã‚’å–å¾—ã™ã‚‹ã‚ˆã†æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
    
    if recall_score < 0.7:
        st.warning("**Context Recall** ãŒä½ã‚ã§ã™ã€‚ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚„æ¤œç´¢å¯¾è±¡æ•°ã‚’å¢—ã‚„ã™ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")

def render_history_section():
    """Render evaluation history"""
    if not st.session_state.eval_history:
        st.info("è©•ä¾¡å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    st.subheader("ğŸ“š è©•ä¾¡å±¥æ­´")
    
    for i, eval_run in enumerate(reversed(st.session_state.eval_history)):
        with st.expander(f"è©•ä¾¡ {len(st.session_state.eval_history) - i}: {eval_run['timestamp'][:19]}"):
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.write(f"**è©•ä¾¡ã‚¿ã‚¤ãƒ—:** {eval_run['eval_type']}")
            with col2:
                st.write(f"**è³ªå•æ•°:** {eval_run['num_questions']}")
            with col3:
                # Calculate average metrics for this run
                results = eval_run['results']
                if results:
                    avg_time = sum(r['response_time'] for r in results) / len(results)
                    st.write(f"**å¹³å‡å¿œç­”æ™‚é–“:** {avg_time:.2f}s")
            
            # Download button for this run
            json_str = json.dumps(eval_run, ensure_ascii=False, indent=2)
            st.download_button(
                label="ğŸ“„ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=json_str,
                file_name=f"evaluation_{eval_run['timestamp'][:10]}.json",
                mime="application/json",
                key=f"download_{i}"
            )

def main():
    """Main function for the Evaluation page"""
    st.set_page_config(
        page_title="Evaluation - Agent RAG Studio",
        page_icon="ğŸ“Š",
        layout="wide"
    )
    
    initialize_session_state()
    
    st.title("ğŸ“Š è©•ä¾¡ãƒ»åˆ†æ")
    st.markdown("RAGã‚·ã‚¹ãƒ†ãƒ ã®æ€§èƒ½ã‚’è©•ä¾¡ã—ã€æ”¹å–„ç‚¹ã‚’ç‰¹å®šã—ã¾ã™")
    
    # Main sections
    render_evaluation_section()
    st.markdown("---")
    render_results_section()
    st.markdown("---")
    render_ragas_results()
    st.markdown("---")
    render_history_section()

if __name__ == "__main__":
    main()
